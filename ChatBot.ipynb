{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784de25e-2db2-46ff-8d14-4d0679c4f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m This environment is externally managed\n",
      "\u001b[31mâ•°â”€>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42889fa6-c18d-4a45-a2b7-c996d862508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit\n",
    "import pickle\n",
    "import time\n",
    "import tqdm as notebook_tqdm\n",
    "import langchain\n",
    "import numpy as np\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2be5ff-d38b-49c9-beb1-a96b524f16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms.base import LLM\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class TransformersLLM(LLM):\n",
    "    def __init__(self, pipe):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, 'pipe', pipe)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"transformers\"\n",
    "\n",
    "    def _call(self, question: str, context: str) -> str:\n",
    "        result = self.pipe(question=question, context=context)\n",
    "        return result['answer']\n",
    "\n",
    "    def _generate(self, prompts: List[str], contexts: List[str], **kwargs) -> List[Dict[str, Any]]:\n",
    "        return [{\"text\": self._call(prompt, context)} for prompt, context in zip(prompts, contexts)]\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipe = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "llm = TransformersLLM(pipe)\n",
    "\n",
    "pipe1 = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
    "llm1 = TransformersLLM(pipe1)\n",
    "\n",
    "# Function to generate a single statement combining the information from chunks\n",
    "def generate_summary_statement(chunks: List[str], question: str, llm: TransformersLLM) -> str:\n",
    "    # print(\"Lookin\")\n",
    "    combined_text = \" \".join(chunks)\n",
    "    prompt = f\"Summarize the following information to answer the question: {question}\\n\\n{combined_text}\"\n",
    "    print(f\"Combined text:\\n{combined_text}\\n\")  # Debug: Check combined text\n",
    "    print(f\"Question: {question}\\n\")  # Debug: Check question\n",
    "    summary = llm._call(prompt, context=combined_text)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8b7be3-5dcc-4b17-a5c5-ad48d1676df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1= 'https://apnews.com/article/debate-trump-biden-cnn-camp-david-florida-2f67b119f58136b310b625e42fad3d3a'\n",
    "url2 = 'https://www.nytimes.com/2024/06/23/us/politics/biden-trump-debate-stakes.html'\n",
    "url3 = 'https://www.yahoo.com/news/biden-trump-presidential-debate-how-to-watch-what-to-know-elections-republican-democrat-cnn-173153159.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93290ea-3b24-4a8d-b67b-7960d5fbcd3e",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcfe57c-ca9c-43b9-8f00-c551c2ceae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = UnstructuredURLLoader(urls = (url1,url2,url3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b81366a-5ad6-4fb3-87d6-c4d14f522877",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21df916-645b-4879-85e3-e4d5711da903",
   "metadata": {},
   "source": [
    "## Document Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f648251c-9a36-4786-aa04-b4e7c856d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 250,\n",
    "        chunk_overlap = 50,\n",
    "        separators = ['\\n\\n','\\n', '.', ' '],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55accf4d-4f8d-469e-bafe-3d02be089579",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286c3d8-e025-4c73-9b95-1de974188e80",
   "metadata": {},
   "source": [
    "## Creating word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ff610c-63ed-4305-a3d8-e32abf756aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "texts = [doc.page_content for doc in docs]\n",
    "embeddings = model.encode(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ec63c9-66e4-48bc-9431-282e1354f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16872ade-e11b-4697-bc2e-0490f4a90856",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = FAISS.from_documents(docs,embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75f2ef-57db-4efe-a8cd-b1c00bebfbfd",
   "metadata": {},
   "source": [
    "## Saving the embeddings file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c245a9a3-47fa-4932-a1ca-d6f43a633adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"vector_index.pkl\"\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(vector_index,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a89654-0c3f-48ff-b08b-2ff97096b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(file_name):\n",
    "    with open(file_name ,'rb') as f:\n",
    "        vector_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988e6dca-dfbd-4fe8-a830-43b4aa2229c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61cc9df5-478f-4df4-b756-4add85ac6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        \n",
    "# # Load QA chain\n",
    "# qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "# qa_with_sources = RetrievalQAWithSourcesChain(combine_documents_chain=qa_chain, retriever=retriever, return_source_documents=True)\n",
    "# result = qa_with_sources._call({\"question\": question_prompt, \"contexts\": docs})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10a978fb-d6e0-475e-a44b-aeaaf4aaeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = \"who are hosting the show\" \n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f87cd373-42f0-405b-a71c-0cd6066a09c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1aa3f7-021f-4f9a-b95b-b8e4f7befc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [chunk.page_content for chunk in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b7e67c0-ae54-48af-8983-2e44b5936e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text:\n",
      "On Saturday, heâ€™s set to host a rally in Philadelphia and deliver a keynote address to a conference of Christian conservatives in Washington. He also has a fundraiser in New Orleans on Monday before going to his Florida estate for meetings. ðŸ—£ï¸ Whoâ€™s moderating the debate?\n",
      "\n",
      "Jake Tapper and Dana Bash, co-hosts of CNNâ€™s Sunday morning show State of the Union, will serve as moderators. Bash, a graduate of George Washington University, anchors CNNâ€™s Inside Politics with Dana Bash and has regularly served as moderator for numerous political town halls and debates â€” including six presidential primary debates in 2016 and two in 2020. âŒš When, where and how to watch\n",
      "\n",
      "The first debate was hosted by CNN at the cable networkâ€™s studios in Atlanta and ran with only two commercial breaks.\n",
      "\n",
      "Yahoo.com will also feature coverage and analysis in real-time from our editorial team.\n",
      "\n",
      "Question: who are hosting the show\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = generate_summary_statement(chunks,question_prompt,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c41cdcd-7551-4b44-932e-157a2455edeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saturday, heâ€™s set to host a rally in Philadelphia'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce5b31ec-2f92-490f-9b76-91e5e50f70ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccde2c6a-2169-4740-ae42-51df557c8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who are hosting the show\"\n",
    "context = \" \".join(chunks)\n",
    "qa_input = {\n",
    "    'question': query,\n",
    "    'context': context\n",
    "}\n",
    "\n",
    "result = qa_pipeline(qa_input)\n",
    "answer = result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56fe9dd1-7f89-4faf-9b6e-5e16318141af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jake Tapper and Dana Bash'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1edf16-a0c6-4563-9487-62f9661b3c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac67e3d-0a14-4096-a663-bc0c01147a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e71115-7829-4885-848b-5db364516564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c8c04-4650-4458-ad74-ef8d225f05af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b1e9d-6364-4945-93e0-25b88a9a961d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d16b1-9f8c-49ef-8027-4871e55ae6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d85de-b124-48bf-bc06-1bc4f1a24558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6eb3e912-be3f-4c29-9db1-ab814792572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings).astype('float32')\n",
    "d = embeddings.shape[1]  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10404bc2-ceb8-4789-875b-495e92299b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the docstore\n",
    "docstore = InMemoryDocstore(dict(enumerate(documents)))\n",
    "\n",
    "# Create FAISS vector store from embeddings\n",
    "vector_store = FAISS(embeddings, index, docstore, dict(enumerate(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54c7afb6-6dd6-4037-a5a3-b9254b4a4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Biden Lost\"\n",
    "query_embedding = model.encode([query]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b9710ab-2b24-45ff-b251-9a1762beefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc2665b5-c452-4192-adc1-328f87f94247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors (indices): [[46  5]]\n",
      "Nearest neighbors (distances): [[35.063553 41.40638 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors (indices): {indices}\")\n",
    "print(f\"Nearest neighbors (distances): {distances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c7ada40-8d53-45eb-aa5f-afe7479cac8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQAWithSourcesChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain/chains/qa_with_sources/base.py:55\u001b[0m, in \u001b[0;36mBaseQAWithSourcesChain.from_llm\u001b[0;34m(cls, llm, document_prompt, question_prompt, combine_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_llm\u001b[39m(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseQAWithSourcesChain:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct the chain from an LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     llm_question_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     llm_combine_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mcombine_prompt)\n\u001b[1;32m     57\u001b[0m     combine_results_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m     58\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mllm_combine_chain,\n\u001b[1;32m     59\u001b[0m         document_prompt\u001b[38;5;241m=\u001b[39mdocument_prompt,\n\u001b[1;32m     60\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummaries\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:203\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     emit_warning()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_llm(llm = pipe, retriever=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31b973-a203-484e-b3c8-d7160ef5a06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chat_env)",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
