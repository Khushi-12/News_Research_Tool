{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784de25e-2db2-46ff-8d14-4d0679c4f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try brew install\n",
      "\u001b[31m   \u001b[0m xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m use a virtual environment:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m python3 -m venv path/to/venv\n",
      "\u001b[31m   \u001b[0m source path/to/venv/bin/activate\n",
      "\u001b[31m   \u001b[0m python3 -m pip install xyz\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. You can install pipx with\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m brew install pipx\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m You may restore the old behavior of pip by passing\n",
      "\u001b[31m   \u001b[0m the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[31m   \u001b[0m 'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[31m   \u001b[0m will permanently disable this error.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[31m   \u001b[0m pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[31m   \u001b[0m file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42889fa6-c18d-4a45-a2b7-c996d862508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit\n",
    "import pickle\n",
    "import time\n",
    "import tqdm as notebook_tqdm\n",
    "import langchain\n",
    "import numpy as np\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2be5ff-d38b-49c9-beb1-a96b524f16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms.base import LLM\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class TransformersLLM(LLM):\n",
    "    def __init__(self, pipe):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, 'pipe', pipe)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"transformers\"\n",
    "\n",
    "    def _call(self, question: str, context: str) -> str:\n",
    "        result = self.pipe(question=question, context=context)\n",
    "        return result['answer']\n",
    "\n",
    "    def _generate(self, prompts: List[str], contexts: List[str], **kwargs) -> List[Dict[str, Any]]:\n",
    "        return [{\"text\": self._call(prompt, context)} for prompt, context in zip(prompts, contexts)]\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipe = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "llm = TransformersLLM(pipe)\n",
    "\n",
    "pipe1 = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
    "llm1 = TransformersLLM(pipe1)\n",
    "\n",
    "# Function to generate a single statement combining the information from chunks\n",
    "def generate_summary_statement(chunks: List[str], question: str, llm: TransformersLLM) -> str:\n",
    "    # print(\"Lookin\")\n",
    "    combined_text = \" \".join(chunks)\n",
    "    prompt = f\"Summarize the following information to answer the question: {question}\\n\\n{combined_text}\"\n",
    "    print(f\"Combined text:\\n{combined_text}\\n\")  # Debug: Check combined text\n",
    "    print(f\"Question: {question}\\n\")  # Debug: Check question\n",
    "    summary = llm._call(prompt, context=combined_text)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8b7be3-5dcc-4b17-a5c5-ad48d1676df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1= 'https://apnews.com/article/debate-trump-biden-cnn-camp-david-florida-2f67b119f58136b310b625e42fad3d3a'\n",
    "url2 = 'https://www.nytimes.com/2024/06/23/us/politics/biden-trump-debate-stakes.html'\n",
    "url3 = 'https://www.yahoo.com/news/biden-trump-presidential-debate-how-to-watch-what-to-know-elections-republican-democrat-cnn-173153159.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93290ea-3b24-4a8d-b67b-7960d5fbcd3e",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcfe57c-ca9c-43b9-8f00-c551c2ceae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = UnstructuredURLLoader(urls = (url1,url2,url3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b81366a-5ad6-4fb3-87d6-c4d14f522877",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21df916-645b-4879-85e3-e4d5711da903",
   "metadata": {},
   "source": [
    "## Document Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f648251c-9a36-4786-aa04-b4e7c856d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 250,\n",
    "        chunk_overlap = 50,\n",
    "        separators = ['\\n\\n','\\n', '.', ' '],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55accf4d-4f8d-469e-bafe-3d02be089579",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286c3d8-e025-4c73-9b95-1de974188e80",
   "metadata": {},
   "source": [
    "## Creating word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ff610c-63ed-4305-a3d8-e32abf756aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "texts = [doc.page_content for doc in docs]\n",
    "embeddings = model.encode(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ec63c9-66e4-48bc-9431-282e1354f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16872ade-e11b-4697-bc2e-0490f4a90856",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = FAISS.from_documents(docs,embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75f2ef-57db-4efe-a8cd-b1c00bebfbfd",
   "metadata": {},
   "source": [
    "## Saving the embeddings file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c245a9a3-47fa-4932-a1ca-d6f43a633adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"vector_index.pkl\"\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(vector_index,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a89654-0c3f-48ff-b08b-2ff97096b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(file_name):\n",
    "    with open(file_name ,'rb') as f:\n",
    "        vector_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988e6dca-dfbd-4fe8-a830-43b4aa2229c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61cc9df5-478f-4df4-b756-4add85ac6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vector_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        \n",
    "# # Load QA chain\n",
    "# qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "# qa_with_sources = RetrievalQAWithSourcesChain(combine_documents_chain=qa_chain, retriever=retriever, return_source_documents=True)\n",
    "# result = qa_with_sources._call({\"question\": question_prompt, \"contexts\": docs})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10a978fb-d6e0-475e-a44b-aeaaf4aaeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = \"who are hosting the show\" \n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f87cd373-42f0-405b-a71c-0cd6066a09c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1aa3f7-021f-4f9a-b95b-b8e4f7befc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [chunk.page_content for chunk in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b7e67c0-ae54-48af-8983-2e44b5936e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text:\n",
      "On Saturday, he’s set to host a rally in Philadelphia and deliver a keynote address to a conference of Christian conservatives in Washington. He also has a fundraiser in New Orleans on Monday before going to his Florida estate for meetings. 🗣️ Who’s moderating the debate?\n",
      "\n",
      "Jake Tapper and Dana Bash, co-hosts of CNN’s Sunday morning show State of the Union, will serve as moderators. Bash, a graduate of George Washington University, anchors CNN’s Inside Politics with Dana Bash and has regularly served as moderator for numerous political town halls and debates — including six presidential primary debates in 2016 and two in 2020. ⌚ When, where and how to watch\n",
      "\n",
      "The first debate was hosted by CNN at the cable network’s studios in Atlanta and ran with only two commercial breaks.\n",
      "\n",
      "Yahoo.com will also feature coverage and analysis in real-time from our editorial team.\n",
      "\n",
      "Question: who are hosting the show\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = generate_summary_statement(chunks,question_prompt,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c41cdcd-7551-4b44-932e-157a2455edeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saturday, he’s set to host a rally in Philadelphia'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce5b31ec-2f92-490f-9b76-91e5e50f70ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khushi/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccde2c6a-2169-4740-ae42-51df557c8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who are hosting the show\"\n",
    "context = \" \".join(chunks)\n",
    "qa_input = {\n",
    "    'question': query,\n",
    "    'context': context\n",
    "}\n",
    "\n",
    "result = qa_pipeline(qa_input)\n",
    "answer = result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56fe9dd1-7f89-4faf-9b6e-5e16318141af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jake Tapper and Dana Bash'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1edf16-a0c6-4563-9487-62f9661b3c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac67e3d-0a14-4096-a663-bc0c01147a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e71115-7829-4885-848b-5db364516564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c8c04-4650-4458-ad74-ef8d225f05af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b1e9d-6364-4945-93e0-25b88a9a961d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d16b1-9f8c-49ef-8027-4871e55ae6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d85de-b124-48bf-bc06-1bc4f1a24558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6eb3e912-be3f-4c29-9db1-ab814792572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings).astype('float32')\n",
    "d = embeddings.shape[1]  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10404bc2-ceb8-4789-875b-495e92299b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the docstore\n",
    "docstore = InMemoryDocstore(dict(enumerate(documents)))\n",
    "\n",
    "# Create FAISS vector store from embeddings\n",
    "vector_store = FAISS(embeddings, index, docstore, dict(enumerate(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54c7afb6-6dd6-4037-a5a3-b9254b4a4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Biden Lost\"\n",
    "query_embedding = model.encode([query]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b9710ab-2b24-45ff-b251-9a1762beefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc2665b5-c452-4192-adc1-328f87f94247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors (indices): [[46  5]]\n",
      "Nearest neighbors (distances): [[35.063553 41.40638 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors (indices): {indices}\")\n",
    "print(f\"Nearest neighbors (distances): {distances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c7ada40-8d53-45eb-aa5f-afe7479cac8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQAWithSourcesChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain/chains/qa_with_sources/base.py:55\u001b[0m, in \u001b[0;36mBaseQAWithSourcesChain.from_llm\u001b[0;34m(cls, llm, document_prompt, question_prompt, combine_prompt, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_llm\u001b[39m(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseQAWithSourcesChain:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct the chain from an LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     llm_question_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     llm_combine_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39mcombine_prompt)\n\u001b[1;32m     57\u001b[0m     combine_results_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m     58\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mllm_combine_chain,\n\u001b[1;32m     59\u001b[0m         document_prompt\u001b[38;5;241m=\u001b[39mdocument_prompt,\n\u001b[1;32m     60\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummaries\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:203\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     emit_warning()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Chat_bot/chat_env/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_llm(llm = pipe, retriever=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31b973-a203-484e-b3c8-d7160ef5a06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chat_env)",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
